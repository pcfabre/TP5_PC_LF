{"cells":[{"cell_type":"markdown","source":["# Movie ratings prediction\n\nOne of the most common uses of big data is to predict what users want.  This allows Google to show you relevant ads, Amazon to recommend relevant products, and Netflix to recommend movies that you might like.  This lab will demonstrate how you can use Apache Spark to recommend movies to a user.  We will start with some basic techniques, and then use the [Spark MLlib][http://spark.apache.org/mllib/] library's Alternating Least Squares method to make more sophisticated predictions.\n\nFor this lab, we will use a subset dataset of 500,000 ratings we included with th VM from the [movielens 10M stable benchmark rating dataset](http://grouplens.org/datasets/movielens/). However, the same code you write will work for the full dataset, or their latest dataset of 21 million ratings.\n\nDuring the second part of the lab, you will submit a job using the full dataset on a Spark cluster deployed on Google Cloud.\n\nThink carefully before calling `collect()` on any datasets.  When you are using a small dataset, calling `collect()` and then using Python to get a sense for the data locally (in the driver program) will work fine, but this will not work when you are using a large dataset that doesn't fit in memory on one machine.  Solutions that call `collect()` and do local analysis that could have been done without Spark will likely fail when running on a cluster."],"metadata":{}},{"cell_type":"code","source":["#Please, run this first\nimport hashlib\nimport sys\n\ndef hash(x):\n  return hashlib.sha1(str(x).encode('utf-8')).hexdigest()\n\nassert sys.version_info.major == 3"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["import os\n\nratings_filename = '/FileStore/tables/ratings_dat-f7611.gz'\nmovies_filename = '/FileStore/tables/movies.dat'"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["We read in each of the files and create an RDD consisting of parsed lines. Each line in the ratings dataset (`ratings.dat.gz`) is formatted as:\n`user_id::movie_id::Rating::Timestamp`\nEach line in the movies (`movies.dat`) dataset is formatted as:\n`movie_id::Title::Genres`\nThe `Genres` field has the format\n`Genres1|Genres2|Genres3|...`\n\nThe format of these files is uniform and simple, so you can use Python [`split()`](https://docs.python.org/2/library/stdtypes.html#str.split) to parse their lines.\n\nParsing the two files yields two RDDS:\n* For each line in the ratings dataset, we create a tuple of (user_id, movie_id, Rating). We drop the timestamp because we do not need it for this exercise.\n* For each line in the movies dataset, we create a tuple of (movie_id, Title). We drop the Genres because we do not need them for this exercise."],"metadata":{}},{"cell_type":"code","source":["num_partitions = 2 # When running on a cluster, \n# you may want to increase this parameter according to the number of executors\nraw_ratings = sc.textFile(ratings_filename).repartition(num_partitions)\nraw_movies = sc.textFile(movies_filename)\n\ndef get_ratings_tuple(entry):\n    \"\"\" Parse a line in the ratings dataset\n    Args:\n        entry (str): a line in the ratings dataset in the form of user_id::movie_id::Rating::Timestamp\n    Returns:\n        tuple: (user_id, movie_id, Rating)\n    \"\"\"\n    items = entry.split('::')\n    return int(items[0]), int(items[1]), float(items[2])\n\n\ndef get_movie_tuple(entry):\n    \"\"\" Parse a line in the movies dataset\n    Args:\n        entry (str): a line in the movies dataset in the form of movie_id::Title::Genres\n    Returns:\n        tuple: (movie_id, Title)\n    \"\"\"\n    items = entry.split('::')\n    return int(items[0]), items[1]\n\n\nratings_RDD = raw_ratings.map(get_ratings_tuple).cache()\nmovies_RDD = raw_movies.map(get_movie_tuple).cache()\n\nratings_count = ratings_RDD.count()\nmovies_count = movies_RDD.count()\n\nprint('There are %s ratings and %s movies in the datasets' % (ratings_count, movies_count))\nprint('Ratings: %s' % ratings_RDD.take(3))\nprint('Movies: %s' % movies_RDD.take(3))\n\nassert ratings_count == 487650\nassert movies_count == 3883\nassert movies_RDD.filter(lambda movie_tuple : movie_tuple[1] == 'Toy Story (1995)').count() == 1\nassert (ratings_RDD.takeOrdered(1, key=lambda rating_tuple: rating_tuple[1]) == [(1, 1, 5.0)])"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">There are 487650 ratings and 3883 movies in the datasets\nRatings: [(1, 1193, 5.0), (1, 661, 3.0), (1, 914, 3.0)]\nMovies: [(1, &#39;Toy Story (1995)&#39;), (2, &#39;Jumanji (1995)&#39;), (3, &#39;Grumpier Old Men (1995)&#39;)]\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["In this lab you will be creating and examining subsets of the tuples, such as the top rated movies by users.\n\nWhenever you examine only a subset of a large dataset, there is the potential that the result will depend on the order you perform operations, such as joins, or how the data is partitioned across the workers. \n\nWe want to guarantee that we always see the same results for a subset, independent of how we manipulate or store the data.\n\nyou can do that by sorting before examining a subset. You might think that the most obvious choice when dealing with an RDD of tuples would be to use the [`sortByKey()` method][sortbykey]. However this choice is problematic, as we can still end up with different results if the key is not unique.\n\n**Note:** It is important to use the [`unicode` type](https://docs.python.org/2/howto/unicode.html#the-unicode-type) instead of the `string` type as the titles are in unicode characters.\n\nConsider the following example, and note that while the sets are equal, the printed lists are usually in different order by value, *although they may randomly match up from time to time.*\nYou can try running this multiple times.  If the last assertion fails, don't worry about it: that was just the luck of the draw.  And note that in some environments, such as this virtual machine, the results are more deterministic than on a cluster with several workers.\n[sortbykey]: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey"],"metadata":{}},{"cell_type":"code","source":["tmp1 = [(1, u'alpha'), (2, u'alpha'), (2, u'beta'), (3, u'alpha'), (1, u'epsilon'), (1, u'delta')]\ntmp2 = [(1, u'delta'), (2, u'alpha'), (2, u'beta'), (3, u'alpha'), (1, u'epsilon'), (1, u'alpha')]\n\none_RDD = sc.parallelize(tmp1)\ntwo_RDD = sc.parallelize(tmp2)\none_sorted = one_RDD.sortByKey(True).collect()\ntwo_sorted = two_RDD.sortByKey(True).collect()\nprint(one_sorted)\nprint(two_sorted)\nassert set(one_sorted) == set(two_sorted)     # Note that both lists have the same elements\nassert two_sorted[0][0] < two_sorted.pop()[0] # Check that it is sorted by the keys\nassert one_sorted[0:2] != two_sorted[0:2]     # Note that the subset consisting of the first two elements does not match"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(1, &#39;alpha&#39;), (1, &#39;epsilon&#39;), (1, &#39;delta&#39;), (2, &#39;alpha&#39;), (2, &#39;beta&#39;), (3, &#39;alpha&#39;)]\n[(1, &#39;delta&#39;), (1, &#39;epsilon&#39;), (1, &#39;alpha&#39;), (2, &#39;alpha&#39;), (2, &#39;beta&#39;), (3, &#39;alpha&#39;)]\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Even though the two lists contain identical tuples, the difference in ordering *sometimes* yields a different ordering for the sorted RDD. If we only examined the first two elements of the RDD (e.g., using `take(2)`), then we would observe different answers - **that is a really bad outcome as we want identical input data to always yield identical output**. \n\nA better technique is to sort the RDD by *both the key and value*, which we can do by combining the key and value into a single string and then sorting on that string. Since the key is an integer and the value is a unicode string, we can use a function to combine them into a single unicode string (e.g., `unicode('%.3f' % key) + ' ' + value`) before sorting the RDD using [sortBy()][sortby].\n[sortby]: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy"],"metadata":{}},{"cell_type":"code","source":["def sort_function(tuple):\n    \"\"\" Construct the sort string (does not perform actual sorting)\n    Args:\n        tuple: (rating, movie_name)\n    Returns:\n        sort_string: the value to sort with, 'rating movie_name'\n    \"\"\"\n    key = str('%.3f' % tuple[0])\n    value = tuple[1]\n    return (key + ' ' + value)\n\n\nprint(one_RDD.sortBy(sort_function, True).collect())\nprint(two_RDD.sortBy(sort_function, True).collect())"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(1, &#39;alpha&#39;), (1, &#39;delta&#39;), (1, &#39;epsilon&#39;), (2, &#39;alpha&#39;), (2, &#39;beta&#39;), (3, &#39;alpha&#39;)]\n[(1, &#39;alpha&#39;), (1, &#39;delta&#39;), (1, &#39;epsilon&#39;), (2, &#39;alpha&#39;), (2, &#39;beta&#39;), (3, &#39;alpha&#39;)]\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["If you just want to look at the first few elements of the RDD in sorted order, you can use the [takeOrdered][takeordered] method with the `sort_function` you defined.\n[takeordered]: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered"],"metadata":{}},{"cell_type":"code","source":["one_sorted1 = one_RDD.takeOrdered(one_RDD.count(),key=sort_function)\ntwo_sorted1 = two_RDD.takeOrdered(two_RDD.count(),key=sort_function)\nprint('one is %s' % one_sorted1)\nprint('two is %s' % two_sorted1)\nassert one_sorted1 == two_sorted1"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">one is [(1, &#39;alpha&#39;), (1, &#39;delta&#39;), (1, &#39;epsilon&#39;), (2, &#39;alpha&#39;), (2, &#39;beta&#39;), (3, &#39;alpha&#39;)]\ntwo is [(1, &#39;alpha&#39;), (1, &#39;delta&#39;), (1, &#39;epsilon&#39;), (2, &#39;alpha&#39;), (2, &#39;beta&#39;), (3, &#39;alpha&#39;)]\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["One way to recommend movies is to always recommend the movies with the highest average rating. \n\nIn this part, you will use Spark to find the name, number of ratings, and the average rating of the 20 movies with the highest average rating and more than 500 reviews. You want to filter out movies with high ratings but fewer than or equal to 500 reviews because movies with few reviews may not have broad appeal to everyone.\n\n\nUsing **only Python**, implement a helper function `get_counts_and_averages()` that takes a single tuple of (movie_id, (rating_1, rating_2, rating_3, ...)) and returns a tuple of (movie_id, (number of ratings, averageRating)). For example, given the tuple `(100, (10.0, 20.0, 30.0))`, your function should return `(100, (3, 20.0))`"],"metadata":{}},{"cell_type":"code","source":["def get_counts_and_averages(id_and_ratings_tuple):\n    \"\"\" Calculate average rating\n    Args:\n        id_and_ratings_tuple: a single tuple of (movie_id, (rating_1, rating_2, rating_3, ...))\n    Returns:\n        tuple: a tuple of (movie_id, (number of ratings, averageRating))\n    \"\"\"\n    item = id_and_ratings_tuple\n    item_tuple =  arranged=(item[0],(len(item[1]),sum(item[1])/float(len(item[1]))))\n    return item_tuple\n\nassert get_counts_and_averages((1, (1, 2, 3, 4))) == (1, (4, 2.5)),\\\n                            'incorrect get_counts_and_averages() with integer list'\nassert get_counts_and_averages((100, (10.0, 20.0, 30.0))) == (100, (3, 20.0)),\\\n                            'incorrect get_counts_and_averages() with float list'\nassert get_counts_and_averages((110, range(20))) == (110, (20, 9.5)),\\\n                            'incorrect get_counts_and_averages() with range'"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Now that we have a way to calculate the average ratings, we will use the `get_counts_and_averages()` helper function with Spark to determine movies with highest average ratings.\nThe steps you should perform are:\n* Recall that the `ratings_RDD` contains tuples of the form (user_id, movie_id, Rating). From `ratings_RDD` create an RDD with tuples of the form (movie_id, Python iterable of Ratings for that movie_id). This transformation will yield an RDD of the form: `[(1, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e7c90>), (2, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e79d0>), (3, <pyspark.resultiterable.ResultIterable object at 0x7f16d50e7610>)]`. Note that you will only need to perform two Spark transformations to do this step.\n* Using `movie_ids_with_ratings_RDD` and your `get_counts_and_averages()` helper function, compute the number of ratings and average rating for each movie to yield tuples of the form (movie_id, (number of ratings, average rating)). This transformation will yield an RDD of the form: `[(1, (993, 4.145015105740181)), (2, (332, 3.174698795180723)), (3, (299, 3.0468227424749164))]`. You can do this step with one Spark transformation\n* We want to see movie names, instead of movie IDs. To `movies_RDD`, apply RDD transformations that use `movie_ids_with_avg_ratings_RDD` to get the movie names for `movie_ids_with_avg_ratings_RDD`, yielding tuples of the form (average rating, movie name, number of ratings). This set of transformations will yield an RDD of the form: `[(1.0, u'Autopsy (Macchie Solari) (1975)', 1), (1.0, u'Better Living (1998)', 1), (1.0, u'Big Squeeze, The (1996)', 3)]`. You will need to do two Spark transformations to complete this step: first use the `movies_RDD` with `movie_ids_with_avg_ratings_RDD` to create a new RDD with Movie names matched to Movie IDs, then convert that RDD into the form of (average rating, movie name, number of ratings). These transformations will yield an RDD that looks like: `[(3.6818181818181817, u'Happiest Millionaire, The (1967)', 22), (3.0468227424749164, u'Grumpier Old Men (1995)', 299), (2.882978723404255, u'Hocus Pocus (1993)', 94)]`"],"metadata":{}},{"cell_type":"code","source":["# From ratings_RDD with tuples of (user_id, movie_id, Rating) create an RDD with tuples of\n# the (movie_id, iterable of Ratings for that movie_id)\nmovie_ids_with_ratings_RDD = (ratings_RDD\n                              .map(lambda x: (x[1],x[2]))\n                              .groupByKey())\nprint('movie_ids_with_ratings_RDD: %s\\n' % movie_ids_with_ratings_RDD.take(3))\n\n# Using `movie_ids_with_ratings_RDD`, compute the number of ratings and average rating for each movie to\n# yield tuples of the form (movie_id, (number of ratings, average rating))\nmovie_ids_with_avg_ratings_RDD = movie_ids_with_ratings_RDD.map(get_counts_and_averages)\nprint('movie_ids_with_avg_ratings_RDD: %s\\n' % movie_ids_with_avg_ratings_RDD.take(3))\n\n# To `movie_ids_with_avg_ratings_RDD`, apply RDD transformations that use `movies_RDD` to get the movie\n# names for `movie_ids_with_avg_ratings_RDD`, yielding tuples of the form\n# (average rating, movie name, number of ratings)\nmovie_name_with_avg_ratings_RDD = (movies_RDD\n                             .join(movie_ids_with_avg_ratings_RDD)\n                             .map(lambda x: x[1])\n                             .map(lambda x: (x[1][1],x[0],x[1][0])))\nprint('movie_name_with_avg_ratings_RDD: %s\\n' % movie_name_with_avg_ratings_RDD.take(3))"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">movie_ids_with_ratings_RDD: [(914, &lt;pyspark.resultiterable.ResultIterable object at 0x7f8c701e7eb8&gt;), (3408, &lt;pyspark.resultiterable.ResultIterable object at 0x7f8c701e7400&gt;), (2804, &lt;pyspark.resultiterable.ResultIterable object at 0x7f8c701e7da0&gt;)]\n\nmovie_ids_with_avg_ratings_RDD: [(914, (314, 4.156050955414012)), (3408, (735, 3.8190476190476192)), (2804, (662, 4.2250755287009065))]\n\nmovie_name_with_avg_ratings_RDD: [(2.676056338028169, &#39;Waiting to Exhale (1995)&#39;, 71), (2.926829268292683, &#39;Tom and Huck (1995)&#39;, 41), (2.3777777777777778, &#39;Dracula: Dead and Loving It (1995)&#39;, 90)]\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["print(ratings_RDD.take(3))\nprint(movie_ids_with_avg_ratings_RDD.take(3))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(1, 1193, 5.0), (1, 661, 3.0), (1, 914, 3.0)]\n[(914, (314, 4.156050955414012)), (3408, (735, 3.8190476190476192)), (2804, (662, 4.2250755287009065))]\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["print(movies_RDD.join(movie_ids_with_avg_ratings_RDD).map(lambda x: x[1]).take(3))\nprint(movies_RDD.join(movie_ids_with_avg_ratings_RDD).map(lambda x: x[1]).map(get_counts_and_averages).take(3))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;Waiting to Exhale (1995)&#39;, (71, 2.676056338028169)), (&#39;Tom and Huck (1995)&#39;, (41, 2.926829268292683)), (&#39;Dracula: Dead and Loving It (1995)&#39;, (90, 2.3777777777777778))]\n[(&#39;Waiting to Exhale (1995)&#39;, (2, 36.83802816901409)), (&#39;Tom and Huck (1995)&#39;, (2, 21.963414634146343)), (&#39;Dracula: Dead and Loving It (1995)&#39;, (2, 46.18888888888889))]\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# test the previous cell\nassert movie_ids_with_ratings_RDD.count() == 3615,\\\n                 'incorrect movie_ids_with_ratings_RDD.count() (expected 3615)'\nmovie_ids_with_ratings_take_ordered = movie_ids_with_ratings_RDD.takeOrdered(3)\n\nassert (movie_ids_with_ratings_take_ordered[0][0] == 1 and\n        len(list(movie_ids_with_ratings_take_ordered[0][1])) == 993),\\\n                'incorrect count of ratings for movie_ids_with_ratings_take_ordered[0] (expected 993)'\n\nassert (movie_ids_with_ratings_take_ordered[1][0] == 2 and\n                len(list(movie_ids_with_ratings_take_ordered[1][1])) == 332),\\\n                'incorrect count of ratings for movie_ids_with_ratings_take_ordered[1] (expected 332)'\n\nassert (movie_ids_with_ratings_take_ordered[2][0] == 3 and\n                len(list(movie_ids_with_ratings_take_ordered[2][1])) == 299),\\\n                'incorrect count of ratings for movie_ids_with_ratings_take_ordered[2] (expected 299)'\n\nassert movie_ids_with_avg_ratings_RDD.count() == 3615,\\\n                'incorrect movie_ids_with_avg_ratings_RDD.count() (expected 3615)'\n\nassert movie_ids_with_avg_ratings_RDD.takeOrdered(3) == \\\n                [(1, (993, 4.145015105740181)), (2, (332, 3.174698795180723)),\n                 (3, (299, 3.0468227424749164))],\\\n                'incorrect movie_ids_with_avg_ratings_RDD.takeOrdered(3)'\n\nassert movie_name_with_avg_ratings_RDD.count() == 3615,\\\n                'incorrect movie_name_with_avg_ratings_RDD.count() (expected 3615)'\n\nassert movie_name_with_avg_ratings_RDD.takeOrdered(3) == \\\n                [(1.0, u'Autopsy (Macchie Solari) (1975)', 1), (1.0, u'Better Living (1998)', 1),\n                 (1.0, u'Big Squeeze, The (1996)', 3)],\\\n                 'incorrect movie_name_with_avg_ratings_RDD.takeOrdered(3)'"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Now that we have an RDD of the movies with highest averge ratings, we can use Spark to determine the 20 movies with highest average ratings and more than 500 reviews.\n\nApply a single RDD transformation to `movie_name_with_avg_ratings_RDD` to limit the results to movies with ratings from more than 500 people. Then, use the `sort_function()` helper function to sort by the average rating to get the movies in order of their rating (highest rating first). You will end up with an RDD of the form: `[(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088), (4.515798462852263, u\"Schindler's List (1993)\", 1171), (4.512893982808023, u'Godfather, The (1972)', 1047)]`"],"metadata":{}},{"cell_type":"code","source":["# Apply an RDD transformation to `movie_name_with_avg_ratings_RDD` to limit the results to movies with\n# ratings from more than 500 people. We then use the `sort_function()` helper function to sort by the\n# average rating to get the movies in order of their rating (highest rating first)\nmovie_limited_and_sorted_by_rating_RDD = (movie_name_with_avg_ratings_RDD\n                                    .filter(lambda x:x[2]>500)\n                                    .sortBy(sort_function, False))\nprint('Movies with highest ratings: %s' % movie_limited_and_sorted_by_rating_RDD.take(20))"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Movies with highest ratings: [(4.5349264705882355, &#39;Shawshank Redemption, The (1994)&#39;, 1088), (4.515798462852263, &#34;Schindler&#39;s List (1993)&#34;, 1171), (4.512893982808023, &#39;Godfather, The (1972)&#39;, 1047), (4.510460251046025, &#39;Raiders of the Lost Ark (1981)&#39;, 1195), (4.505415162454874, &#39;Usual Suspects, The (1995)&#39;, 831), (4.457256461232604, &#39;Rear Window (1954)&#39;, 503), (4.45468509984639, &#39;Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)&#39;, 651), (4.43953006219765, &#39;Star Wars: Episode IV - A New Hope (1977)&#39;, 1447), (4.4, &#39;Sixth Sense, The (1999)&#39;, 1110), (4.394285714285714, &#39;North by Northwest (1959)&#39;, 700), (4.379506641366224, &#39;Citizen Kane (1941)&#39;, 527), (4.375, &#39;Casablanca (1942)&#39;, 776), (4.363975155279503, &#39;Godfather: Part II, The (1974)&#39;, 805), (4.358816276202219, &#34;One Flew Over the Cuckoo&#39;s Nest (1975)&#34;, 811), (4.358173076923077, &#39;Silence of the Lambs, The (1991)&#39;, 1248), (4.335826477187734, &#39;Saving Private Ryan (1998)&#39;, 1337), (4.326241134751773, &#39;Chinatown (1974)&#39;, 564), (4.325383304940375, &#39;Life Is Beautiful (La Vita � bella) (1997)&#39;, 587), (4.324110671936759, &#39;Monty Python and the Holy Grail (1974)&#39;, 759), (4.3096, &#39;Matrix, The (1999)&#39;, 1250)]\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["assert movie_limited_and_sorted_by_rating_RDD.count() == 194,\\\n                'incorrect movie_limited_and_sorted_by_rating_RDD.count()'\n\nassert movie_limited_and_sorted_by_rating_RDD.take(20) == \\\n              [(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088),\n               (4.515798462852263, u\"Schindler's List (1993)\", 1171),\n               (4.512893982808023, u'Godfather, The (1972)', 1047),\n               (4.510460251046025, u'Raiders of the Lost Ark (1981)', 1195),\n               (4.505415162454874, u'Usual Suspects, The (1995)', 831),\n               (4.457256461232604, u'Rear Window (1954)', 503),\n               (4.45468509984639, u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)', 651),\n               (4.43953006219765, u'Star Wars: Episode IV - A New Hope (1977)', 1447),\n               (4.4, u'Sixth Sense, The (1999)', 1110), (4.394285714285714, u'North by Northwest (1959)', 700),\n               (4.379506641366224, u'Citizen Kane (1941)', 527), (4.375, u'Casablanca (1942)', 776),\n               (4.363975155279503, u'Godfather: Part II, The (1974)', 805),\n               (4.358816276202219, u\"One Flew Over the Cuckoo's Nest (1975)\", 811),\n               (4.358173076923077, u'Silence of the Lambs, The (1991)', 1248),\n               (4.335826477187734, u'Saving Private Ryan (1998)', 1337),\n               (4.326241134751773, u'Chinatown (1974)', 564),\n               (4.325383304940375, u'Life Is Beautiful (La Vita \\ufffd bella) (1997)', 587),\n               (4.324110671936759, u'Monty Python and the Holy Grail (1974)', 759),\n               (4.3096, u'Matrix, The (1999)', 1250)],\\\n        'incorrect sortedByRating_RDD.take(20)'"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["Using a threshold on the number of reviews is one way to improve the recommendations, but there are many other good ways to improve quality. For example, you could weight ratings by the number of ratings.\n\nIn this practical, you will learn how to use MLlib to make personalized movie recommendations using the movie data we have been analyzing.\n\nYou are going to use a technique called [collaborative filtering][collab]. Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. You can read more about collaborative filtering [here][collab2].\n\nThe image below (from [Wikipedia][collab]) shows an example of predicting of the user's rating using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image below the system has made a prediction, that the active user will not like the video.\n![collaborative filtering](https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif)\n[mllib]: https://spark.apache.org/mllib/\n[collab]: https://en.wikipedia.org/?title=Collaborative_filtering\n[collab2]: http://recommender-systems.org/collaborative-filtering/"],"metadata":{}},{"cell_type":"markdown","source":["For movie recommendations, we start with a matrix whose entries are movie ratings by users (shown in red in the diagram below).  Each column represents a user (shown in green) and each row represents a particular movie (shown in blue).\n\nSince not all users have rated all movies, we do not know all of the entries in this matrix, which is precisely why we need collaborative filtering.  For each user, we have ratings for only a subset of the movies.  With collaborative filtering, the idea is to approximate the ratings matrix by factorizing it as the product of two matrices: one that describes properties of each user (shown in green), and one that describes properties of each movie (shown in blue).\n![factorization](http://spark-mooc.github.io/web-assets/images/matrix_factorization.png)\n\nWe want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized.  The [Alternating Least Squares][als] algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constrant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name.\n\nThis optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n\n[als]: https://en.wikiversity.org/wiki/Least-Squares_Method\n\nBefore you jump into using machine learning, we need to break up the `ratings_RDD` dataset into three pieces:\n* A training set (RDD), which we will use to train models\n* A validation set (RDD), which we will use to choose the best model\n* A test set (RDD), which we will use for our experiments\n\nTo randomly split the dataset into the multiple groups, we can use the pySpark [randomSplit()](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) transformation. `randomSplit()` takes a set of splits and and seed and returns multiple RDDs."],"metadata":{}},{"cell_type":"code","source":["training_RDD, validation_RDD, test_RDD = ratings_RDD.randomSplit([6, 2, 2], seed=0)\n\nprint('Training: %s, validation: %s, test: %s\\n' % (training_RDD.count(),\n                                                    validation_RDD.count(),\n                                                    test_RDD.count()))\nprint(training_RDD.take(3))\nprint(validation_RDD.take(3))\nprint(test_RDD.take(3))\n\nassert training_RDD.count() == 293180\nassert validation_RDD.count() == 96898\nassert test_RDD.count() == 97572\n\nassert training_RDD.filter(lambda t: t == (1, 1193, 5.0)).count() == 1\nassert training_RDD.filter(lambda t: t == (1, 661, 3.0)).count() == 1\nassert training_RDD.filter(lambda t: t == (1, 2355, 5.0)).count() == 1\n\nassert validation_RDD.filter(lambda t: t == (1, 3408, 4.0)).count() == 1\nassert validation_RDD.filter(lambda t: t == (1, 914, 3.0)).count() == 1\nassert validation_RDD.filter(lambda t: t == (1, 2321, 3.0)).count() == 1\n\nassert test_RDD.filter(lambda t: t == (1, 1197, 3.0)).count() == 1\nassert test_RDD.filter(lambda t: t == (1, 1287, 5.0)).count() == 1\nassert test_RDD.filter(lambda t: t == (1, 2804, 5.0)).count() == 1"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training: 293180, validation: 96898, test: 97572\n\n[(1, 1193, 5.0), (1, 661, 3.0), (1, 2355, 5.0)]\n[(1, 914, 3.0), (1, 3408, 4.0), (1, 2321, 3.0)]\n[(1, 1197, 3.0), (1, 1287, 5.0), (1, 2804, 5.0)]\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["After splitting the dataset, your training set has about 293,000 entries and the validation and test sets each have about 97,000 entries (the exact number of entries in each dataset varies slightly due to the random nature of the `randomSplit()` transformation.\n\nIn the next part, you will generate a few different models, and will need a way to decide which model is best. We will use the [Root Mean Square Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) or Root Mean Square Deviation (RMSD) to compute the error of each model. \n\nRMSE is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. The RMSE serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSE is a good measure of accuracy, but only to compare forecasting errors of different models for a particular variable and not between variables, as it is scale-dependent.\n\n The RMSE is the square root of the average value of the square of `(actual rating - predicted rating)` for all users and movies for which we have the actual rating. Versions of Spark MLlib beginning with Spark 1.4 include a [RegressionMetrics](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RegressionMetrics) module that can be used to compute the RMSE. However, you are here to learn, you will write your own function :)\n \nWrite a function to compute the sum of squared error given `predicted_RDD` and `actual_RDD` RDDs. Both RDDs consist of tuples of the form (user_id, movie_id, Rating)\n\nGiven two ratings RDDs, *x* and *y* of size *n*, we define RSME as follows: $ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}$\n\nTo calculate RSME, the steps you should perform are:\n* Transform `predicted_RDD` into the tuples of the form ((user_id, movie_id), Rating). For example, tuples like `[((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3), ((2, 2), 2), ((2, 3), 4)]`. You can perform this step with a single Spark transformation.\n* Transform `actual_RDD` into the tuples of the form ((user_id, movie_id), Rating). For example, tuples like `[((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)]`. You can perform this step with a single Spark transformation.\n* Using only RDD transformations (you only need to perform two transformations), compute the squared error for each *matching* entry (i.e., the same (user_id, movie_id) in each RDD) in the reformatted RDDs - do *not* use `collect()` to perform this step. Note that not every (user_id, movie_id) pair will appear in both RDDs - if a pair does not appear in both RDDs, then it does not contribute to the RMSE. You will end up with an RDD with entries of the form $ (x_i - y_i)^2$ You might want to check out Python's [math](https://docs.python.org/2/library/math.html) module to see how to compute these values\n* Using an RDD action (but **not** `collect()`), compute the total squared error: $ SE = \\sum_{i = 1}^{n} (x_i - y_i)^2 $\n* Compute *n* by using an RDD action (but **not** `collect()`), to count the number of pairs for which you computed the total squared error\n* Using the total squared error and the number of pairs, compute the RSME. Make sure you compute this value as a [float](https://docs.python.org/2/library/stdtypes.html#numeric-types-int-float-long-complex).\n\nNote: Your solution must only use transformations and actions on RDDs. Do _not_ call `collect()` on either RDD."],"metadata":{}},{"cell_type":"code","source":["import math\n\ndef compute_error(predicted_RDD, actual_RDD):\n    \"\"\" Compute the root mean squared error between predicted and actual\n    Args:\n        predicted_RDD: predicted ratings for each movie and each user where each entry is in the form\n                      (user_id, movie_id, Rating)\n        actual_RDD: actual ratings where each entry is in the form (user_id, movie_id, Rating)\n    Returns:\n        RSME (float): computed RSME value\n    \"\"\"\n    # Transform predicted_RDD into the tuples of the form ((user_id, movie_id), Rating)\n    predicted_reformatted_RDD = predicted_RDD.map(lambda x: ((x[0],x[1]),x[2]))\n\n    # Transform actual_RDD into the tuples of the form ((user_id, movie_id), Rating)\n    actual_reformatted_RDD = actual_RDD.map(lambda x: ((x[0],x[1]),x[2]))\n\n    # Compute the squared error for each matching entry (i.e., the same (User ID, Movie ID) in each\n    # RDD) in the reformatted RDDs using RDD transformtions - do not use collect()\n    squared_errors_RDD = (predicted_reformatted_RDD.join(actual_reformatted_RDD)\n                        .map(lambda x: (x[1][0]-x[1][1])**2))\n\n\n    # Compute the total squared error - do not use collect()\n    total_error = squared_errors_RDD.reduce(lambda x,y:x+y)\n\n    # Count the number of entries for which you computed the total squared error\n    num_ratings = squared_errors_RDD.count()\n\n    # Using the total squared error and the number of entries, compute the RSME\n    return (total_error/float(num_ratings))**0.5\n\n\n# sc.parallelize turns a Python list into a Spark RDD.\ntest_predicted = sc.parallelize([\n    (1, 1, 5),\n    (1, 2, 3),\n    (1, 3, 4),\n    (2, 1, 3),\n    (2, 2, 2),\n    (2, 3, 4)])\ntest_actual = sc.parallelize([\n     (1, 2, 3),\n     (1, 3, 5),\n     (2, 1, 5),\n     (2, 2, 1)])\ntest_predicted2 = sc.parallelize([\n     (2, 2, 5),\n     (1, 2, 5)])\ntest_error = compute_error(test_predicted, test_actual)\nprint('RMSE for test dataset (should be 1.22474487139): %s' % test_error)\n\ntest_error2 = compute_error(test_predicted2, test_actual)\nprint('RMSE for test dataset2 (should be 3.16227766017): %s' % test_error2)\n\ntest_error3 = compute_error(test_actual, test_actual)\nprint('RMSE for test_actual dataset (should be 0.0): %s' % test_error3)\n\nassert abs(test_error - 1.22474487139) < 0.00000001,\\\n                'incorrect test_error (expected 1.22474487139)'\nassert abs(test_error2 - 3.16227766017) < 0.00000001,\\\n                'incorrect test_error2 result (expected 3.16227766017)'\nassert abs(test_error3 - 0.0) < 0.00000001,\\\n                'incorrect test_actual result (expected 0.0)'"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">RMSE for test dataset (should be 1.22474487139): 1.224744871391589\nRMSE for test dataset2 (should be 3.16227766017): 3.1622776601683795\nRMSE for test_actual dataset (should be 0.0): 0.0\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["You will now use the MLlib implementation of Alternating Least Squares, [ALS.train()](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS). ALS takes a training dataset (RDD) and several parameters that control the model creation process. To determine the best values for the parameters, you will use ALS to train several models, and then you will select the best model and use the parameters from that model in the rest of this lab exercise.\n\nThe process you will use for determining the best model is as follows:\n* Pick a set of model parameters. The most important parameter to `ALS.train()` is the *rank*, which is the number of rows in the Users matrix (green in the diagram above) or the number of columns in the Movies matrix (blue in the diagram above). (In general, a lower rank will mean higher error on the training dataset, but a high rank may lead to [overfitting](https://en.wikipedia.org/wiki/Overfitting).)  You will train models with ranks of 4, 8, and 12 using the `training_RDD` dataset.\n* Create a model using `ALS.train(training_RDD, rank, seed=seed, iterations=iterations, lambda_=regularization_parameter)` with three parameters: an RDD consisting of tuples of the form (user_id, movie_id, rating) used to train the model, an integer rank (4, 8, or 12), a number of iterations to execute (we will use 5 for the `iterations` parameter), and a regularization coefficient (we will use 0.1 for the `regularization_parameter`).\n* For the prediction step, create an input RDD, `validation_for_predict_RDD`, consisting of (user_id, movie_id) pairs that you extract from `validation_RDD`. You will end up with an RDD of the form: `[(1, 1287), (1, 594), (1, 1270)]`\n* Using the model and `validation_for_predict_RDD`, you can predict rating values by calling [model.predictAll()](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.MatrixFactorizationModel.predictAll) with the `validation_for_predict_RDD` dataset, where `model` is the model we generated with ALS.train().  `predictAll` accepts an RDD with each entry in the format (user_id, movie_id) and outputs an RDD with each entry in the format (user_id, movie_id, rating).\n* Evaluate the quality of the model by using the `compute_error()` function you wrote ealier to compute the error between the predicted ratings and the actual ratings in `validation_RDD`.\n\nNote: It is likely that this operation will take a noticeable amount of time (around a minute in our VM depending on your hardware); you can observe its progress on the [Spark Web UI](http://localhost:4040). Probably most of the time will be spent running your `compute_error()` function, since, unlike the Spark ALS implementation (and the Spark 1.4 [RegressionMetrics](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RegressionMetrics) module), this does not use a fast linear algebra library and needs to run some Python code for all 100k entries."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.recommendation import ALS\n\nvalidation_for_predict_RDD = validation_RDD.map(lambda x: (x[0],x[1]))\n\nseed = 5\niterations = 5\nregularization_parameter = 0.1\nranks = [4, 8, 12]\nerrors = [0, 0, 0]\nerr = 0\ntolerance = 0.03\n\nmin_error = float('inf')\nbest_rank = -1\nfor rank in ranks:\n    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n                      lambda_=regularization_parameter)\n    predicted_ratings_RDD = model.predictAll(validation_for_predict_RDD)\n    error = compute_error(predicted_ratings_RDD, validation_RDD)\n    errors[err] = error\n    err += 1\n    print('For rank %s the RMSE is %s' % (rank, error))\n    if error < min_error:\n        min_error = error\n        best_rank = rank\n\nprint('The best model was trained with rank %s' % best_rank)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">For rank 4 the RMSE is 0.8960745588092996\nFor rank 8 the RMSE is 0.8949686142293647\nFor rank 12 the RMSE is 0.891538951438127\nThe best model was trained with rank 12\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["assert training_RDD.getNumPartitions() == 2,\\\n                  'incorrect number of partitions for training_RDD (expected 2)'\nassert validation_for_predict_RDD.count() == 96898,\\\n                  'incorrect size for validation_for_predict_RDD (expected 96898)'\nassert validation_for_predict_RDD.filter(lambda t: t == (1, 3408)).count() == 1,\\\n                  'incorrect content for validation_for_predict_RDD'\nassert abs(errors[0] - 0.895405660311) < tolerance, 'incorrect errors[0]'\nassert abs(errors[1] - 0.895514822303) < tolerance, 'incorrect errors[1]'\nassert abs(errors[2] - 0.894980442967) < tolerance, 'incorrect errors[2]'"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["So far, you used the `training_RDD` and `validation_RDD` datasets to select the best model.  Since we used these two datasets to determine what model is best, we cannot use them to test how good the model is - otherwise we would be very vulnerable to [overfitting](https://en.wikipedia.org/wiki/Overfitting).  To decide how good our model is, we need to use the `test_RDD` dataset.  We will use the `best_rank` you determined in part earlier to create a model for predicting the ratings for the test dataset and then we will compute the RMSE.\n\nYou should perform the following steps:\n* Train a model, using the `training_RDD`, `best_rank` and the parameters you used from earlier: `seed=seed`, `iterations=iterations`, and `lambda_=regularization_parameter` - make sure you include **all** of the parameters.\n* For the prediction step, create an input RDD, `test_for_predicting_RDD`, consisting of (user_id, movie_id) pairs that you extract from `test_RDD`. You will end up with an RDD of the form: `[(1, 1287), (1, 594), (1, 1270)]`\n* Use [my_model.predictAll()](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.MatrixFactorizationModel.predictAll) to predict rating values for the test dataset.\n* For validation, use the `test_RDD`and your `compute_error` function to compute the RMSE between `test_RDD` and the `predicted_test_RDD` from the model.\n* Evaluate the quality of the model by using the `compute_error()` function you wrote in part (2b) to compute the error between the predicted ratings and the actual ratings in `test_RDD`."],"metadata":{}},{"cell_type":"code","source":["my_model = ALS.train(training_RDD, best_rank, seed=5, iterations=5,\n                      lambda_=0.1)\ntest_for_predicting_RDD = test_RDD.map(lambda x: (x[0],x[1]))\n\npredicted_test_RDD = my_model.predictAll(test_for_predicting_RDD)\n\ntest_RMSE = compute_error(test_RDD, predicted_test_RDD)\n\nprint('The model had a RMSE on the test set of %s' % test_RMSE)\n\nassert abs(test_RMSE - 0.896040796967) < tolerance, 'incorrect test_RMSE'"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The model had a RMSE on the test set of 0.8933944907915308\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["Looking at the RMSE for the results predicted by the model versus the values in the test set is one way to evalute the quality of our model. Another way to evaluate the model is to evaluate the error from a test set where every rating is the average rating for the training set.\n\nYou should perform the following steps:\n* Use the `training_RDD` to compute the average rating across all movies in that training dataset.\n* Use the average rating that you just determined and the `test_RDD` to create an RDD with entries of the form (user_id, movie_id, average rating).\n* Use your `compute_error` function to compute the RMSE between the `test_RDD` validation RDD that you just created and the `test_for_avg_RDD`."],"metadata":{}},{"cell_type":"code","source":["training_avg_rating = training_RDD.map(lambda (x,y,z):z).reduce(lambda x,y:x+y)/float(training_RDD.count())\nprint('The average rating for movies in the training set is %s' % training_avg_rating)\n\ntest_for_avg_RDD = test_RDD.map(lambda (x,y,z):(x,y,training_avg_rating))\ntest_avg_RMSE = compute_error(test_RDD, test_for_avg_RDD)\nprint('The RMSE on the average set is %s' % test_avg_RMSE)\n\nassert abs(training_avg_rating - 3.5716010641) < 0.000001,\\\n                'incorrect training_avg_rating (expected 3.5716010641)'\nassert abs(test_avg_RMSE - 1.11441205015) < 0.000001,\\\n                'incorrect test_avg_RMSE (expected 1.11441205015)'"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-3168043217108004&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">1</span>\n<span class=\"ansi-red-fg\">    training_avg_rating = training_RDD.map(lambda (x,y,z):z).reduce(lambda x,y:x+y)/float(training_RDD.count())</span>\n                                                  ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> invalid syntax\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["You now have code to predict how users will rate movies!\n\nThe ultimate goal of this exercise is to predict what movies to recommend to yourself. In order to do that, you will first need to add ratings for yourself to the `ratings_RDD` dataset.\n\nTo help you provide ratings for yourself, use the following code cell to list the names and movie IDs of the 50 highest-rated movies from `movie_limited_and_sorted_by_rating_RDD` which we created in part 1 the lab."],"metadata":{}},{"cell_type":"code","source":["print('Most rated movies:')\nprint('(average rating, movie name, number of reviews)')\nfor ratings_tuple in movie_limited_and_sorted_by_rating_RDD.take(50):\n    print(ratings_tuple)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Most rated movies:\n(average rating, movie name, number of reviews)\n(4.5349264705882355, &#39;Shawshank Redemption, The (1994)&#39;, 1088)\n(4.515798462852263, &#34;Schindler&#39;s List (1993)&#34;, 1171)\n(4.512893982808023, &#39;Godfather, The (1972)&#39;, 1047)\n(4.510460251046025, &#39;Raiders of the Lost Ark (1981)&#39;, 1195)\n(4.505415162454874, &#39;Usual Suspects, The (1995)&#39;, 831)\n(4.457256461232604, &#39;Rear Window (1954)&#39;, 503)\n(4.45468509984639, &#39;Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)&#39;, 651)\n(4.43953006219765, &#39;Star Wars: Episode IV - A New Hope (1977)&#39;, 1447)\n(4.4, &#39;Sixth Sense, The (1999)&#39;, 1110)\n(4.394285714285714, &#39;North by Northwest (1959)&#39;, 700)\n(4.379506641366224, &#39;Citizen Kane (1941)&#39;, 527)\n(4.375, &#39;Casablanca (1942)&#39;, 776)\n(4.363975155279503, &#39;Godfather: Part II, The (1974)&#39;, 805)\n(4.358816276202219, &#34;One Flew Over the Cuckoo&#39;s Nest (1975)&#34;, 811)\n(4.358173076923077, &#39;Silence of the Lambs, The (1991)&#39;, 1248)\n(4.335826477187734, &#39;Saving Private Ryan (1998)&#39;, 1337)\n(4.326241134751773, &#39;Chinatown (1974)&#39;, 564)\n(4.325383304940375, &#39;Life Is Beautiful (La Vita � bella) (1997)&#39;, 587)\n(4.324110671936759, &#39;Monty Python and the Holy Grail (1974)&#39;, 759)\n(4.3096, &#39;Matrix, The (1999)&#39;, 1250)\n(4.309457579972183, &#39;Star Wars: Episode V - The Empire Strikes Back (1980)&#39;, 1438)\n(4.30379746835443, &#39;Young Frankenstein (1974)&#39;, 553)\n(4.301346801346801, &#39;Psycho (1960)&#39;, 594)\n(4.296438883541867, &#39;Pulp Fiction (1994)&#39;, 1039)\n(4.286535303776683, &#39;Fargo (1996)&#39;, 1218)\n(4.282367447595561, &#39;GoodFellas (1990)&#39;, 811)\n(4.27943661971831, &#39;American Beauty (1999)&#39;, 1775)\n(4.268053855569155, &#39;Wizard of Oz, The (1939)&#39;, 817)\n(4.267774699907664, &#39;Princess Bride, The (1987)&#39;, 1083)\n(4.253333333333333, &#39;Graduate, The (1967)&#39;, 600)\n(4.236263736263736, &#39;Run Lola Run (Lola rennt) (1998)&#39;, 546)\n(4.233807266982622, &#39;Amadeus (1984)&#39;, 633)\n(4.232558139534884, &#39;Toy Story 2 (1999)&#39;, 860)\n(4.232558139534884, &#39;This Is Spinal Tap (1984)&#39;, 516)\n(4.228494623655914, &#39;Almost Famous (2000)&#39;, 744)\n(4.2250755287009065, &#39;Christmas Story, A (1983)&#39;, 662)\n(4.216757741347905, &#39;Glory (1989)&#39;, 549)\n(4.213358070500927, &#39;Apocalypse Now (1979)&#39;, 539)\n(4.20992028343667, &#39;L.A. Confidential (1997)&#39;, 1129)\n(4.204733727810651, &#39;Blade Runner (1982)&#39;, 845)\n(4.1886120996441285, &#39;Sling Blade (1996)&#39;, 562)\n(4.184615384615385, &#39;Braveheart (1995)&#39;, 1300)\n(4.184168012924071, &#39;Butch Cassidy and the Sundance Kid (1969)&#39;, 619)\n(4.182509505703422, &#39;Good Will Hunting (1997)&#39;, 789)\n(4.166969147005445, &#39;Taxi Driver (1976)&#39;, 551)\n(4.162767039674466, &#39;Terminator, The (1984)&#39;, 983)\n(4.157545605306799, &#39;Reservoir Dogs (1992)&#39;, 603)\n(4.153333333333333, &#39;Jaws (1975)&#39;, 750)\n(4.149840595111583, &#39;Alien (1979)&#39;, 941)\n(4.145015105740181, &#39;Toy Story (1995)&#39;, 993)\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["The user ID 0 is unassigned, so you will use it for your ratings. Set the variable `myuser_id` to 0. \n\nNext, create a new RDD `my_ratings_RDD` with your ratings for at least 10 movie ratings. Each entry should be formatted as `(myuser_id, movie_id, rating)` (i.e., each entry should be formatted in the same way as `training_RDD`).  As in the original dataset, ratings should be between 1 and 5 (inclusive). If you have not seen at least 10 of these movies, you can increase the parameter passed to `take()` in the above cell until there are 10 movies that you have seen (or you can also guess what your rating would be for movies you have not seen)."],"metadata":{}},{"cell_type":"code","source":["training_RDD"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[173]: PythonRDD[2535] at RDD at PythonRDD.scala:59</div>"]}}],"execution_count":37},{"cell_type":"code","source":["myuser_id = 0\n\n# Note that the movie IDs are the *last* number on each line.\nmy_rated_movies = [\n     # The format of each line is (myuser_id, movie ID, your rating)\n     # For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\" \n     # a five rating, you would add the following line:\n     #   (myuser_id, 260, 5),\n    (myuser_id,1013,2),(myuser_id,1410,4),(myuser_id,830,6),(myuser_id,860,3)\n    ,(myuser_id,112,4),(myuser_id,247,5),(myuser_id,269,5),(myuser_id,600,5),(myuser_id,624,5),\n    (myuser_id,700,3)\n    ]\nmy_ratings_RDD = sc.parallelize(my_rated_movies)\nprint('My movie ratings: %s' % my_ratings_RDD.take(10))"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">My movie ratings: [(0, 1013, 2), (0, 1410, 4), (0, 830, 6), (0, 860, 3), (0, 112, 4), (0, 247, 5), (0, 269, 5), (0, 600, 5), (0, 624, 5), (0, 700, 3)]\n</div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["Now that you have ratings for yourself, you need to add your ratings to the `training` dataset so that the model you train will incorporate your preferences.  Spark's [union()](http://spark.apache.org/docs/latest/api/python/pyspark.rdd.RDD-class.html#union) transformation combines two RDDs; use `union()` to create a new training dataset that includes your ratings and the data in the original training dataset."],"metadata":{}},{"cell_type":"code","source":["training_with_my_ratings_RDD = training_RDD.union(my_ratings_RDD)\n\nprint('The training dataset now has %s more entries than the original training dataset' %\n       (training_with_my_ratings_RDD.count() - training_RDD.count()))\nassert (training_with_my_ratings_RDD.count() - training_RDD.count()) == my_ratings_RDD.count()"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The training dataset now has 10 more entries than the original training dataset\n</div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["Now, train a model with your ratings added and the parameters you used in in part (2c): `best_rank`, `seed=seed`, `iterations=iterations`, and `lambda_=regularization_parameter` - make sure you include **all** of the parameters."],"metadata":{}},{"cell_type":"code","source":["my_ratings_model = ALS.train(training_with_my_ratings_RDD, best_rank, seed=5, iterations=5,\n                      lambda_=0.1)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["Compute the RMSE for this new model on the test set.\n* For the prediction step, reuse `test_for_predicting_RDD`, consisting of (user_id, movie_id) pairs that you extracted from `test_RDD`. The RDD has the form: `[(1, 1287), (1, 594), (1, 1270)]`\n* Use `my_ratings_model.predictAll()` to predict rating values for the `test_for_predicting_RDD` test dataset, set this as `predictedtest_my_ratings_RDD`\n* For validation, use the `test_RDD`and your `compute_error` function to compute the RMSE between `test_RDD` and the `predictedtest_my_ratings_RDD` from the model."],"metadata":{}},{"cell_type":"code","source":["predictedtest_my_ratings_RDD = my_ratings_model.predictAll(test_for_predicting_RDD)\ntest_RMSE_my_ratings = compute_error(test_RDD, predictedtest_my_ratings_RDD)\nprint('The model had a RMSE on the test set of %s' % test_RMSE_my_ratings)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The model had a RMSE on the test set of 0.8911437158828065\n</div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["So far, we have only used the `predictAll` method to compute the error of the model.  Here, use the `predictAll` to predict what ratings you would give to the movies that you did not already provide ratings for.\n\nYou should perform the following steps:\n* Use the Python list `my_rated_movies` to transform the `movies_RDD` into an RDD with entries that are pairs of the form (myuser_id, Movie ID) and that does not contain any movies that you have rated. This transformation will yield an RDD of the form: `[(0, 1), (0, 2), (0, 3), (0, 4)]`. Note that you can do this step with one RDD transformation.\n* For the prediction step, use the input RDD, `my_unrated_movies_RDD`, with my_ratings_model.predictAll() to predict your ratings for the movies."],"metadata":{}},{"cell_type":"code","source":["movies_RDD.take(5)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[178]: [(1, &#39;Toy Story (1995)&#39;),\n (2, &#39;Jumanji (1995)&#39;),\n (3, &#39;Grumpier Old Men (1995)&#39;),\n (4, &#39;Waiting to Exhale (1995)&#39;),\n (5, &#39;Father of the Bride Part II (1995)&#39;)]</div>"]}}],"execution_count":46},{"cell_type":"code","source":["# Use the Python list my_rated_movies to transform the movies_RDD into an RDD with entries that are \n# pairs of the form (myuser_id, Movie ID) and that does not contain any movies that you have rated.\nmy_rated_movies_id = [x[1] for x in my_rated_movies]\nmy_unrated_movies_RDD = (movies_RDD\n                         .filter(lambda x: x[0] not in my_rated_movies_id)\n                         .map(lambda x:(0,x[0]))\n                         )\n\n# Use the input RDD, my_unrated_movies_RDD, with my_ratings_model.predictAll() to predict your ratings for the movies\npredicted_ratings_RDD = my_ratings_model.predictAll(my_unrated_movies_RDD)"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":47},{"cell_type":"markdown","source":["You have our predicted ratings. Now you can print out the 25 movies with the highest predicted ratings.\n\nYou should perform the following steps:\n* From earlier, you know that you should look at movies with a reasonable number of reviews (e.g., more than 75 reviews). You can experiment with a lower threshold, but fewer ratings for a movie may yield higher prediction errors. Transform `movie_ids_with_avg_ratings_RDD`, which has the form (movie_id, (number of ratings, average rating)), into an RDD of the form (movie_id, number of ratings): `[(2, 332), (4, 71), (6, 442)]`\n* We want to see movie names, instead of movie IDs. Transform `predicted_ratings_RDD` into an RDD with entries that are pairs of the form (Movie ID, Predicted Rating): `[(3456, -0.5501005376936687), (1080, 1.5885892024487962), (320, -3.7952255522487865)]`\n* Use RDD transformations with `predicted_RDD` and `movie_counts_RDD` to yield an RDD with tuples of the form (Movie ID, (Predicted Rating, number of ratings)): `[(2050, (0.6694097486155939, 44)), (10, (5.29762541533513, 418)), (2060, (0.5055259373841172, 97))]`\n* Use RDD transformations with `predicted_with_counts_RDD` and `movies_RDD` to yield an RDD with tuples of the form (Predicted Rating, Movie Name, number of ratings), _for movies with more than 75 ratings._ For example: `[(7.983121900375243, u'Under Siege (1992)'), (7.9769201864261285, u'Fifth Element, The (1997)')]`"],"metadata":{}},{"cell_type":"code","source":["# Transform movie_ids_with_avg_ratings_RDD from part (1b), which has the form (movie_id, (number of ratings, average rating)),\n# into and RDD of the form (movie_id, number of ratings)\nmovie_counts_RDD = movie_ids_with_avg_ratings_RDD.map(lambda x: (x[0],x[1][0]))\n\n# Transform predicted_ratings_RDD into an RDD with entries that are pairs of the form (Movie ID, Predicted Rating)\npredicted_RDD = predicted_ratings_RDD.map(lambda x: (x[1],x[2]))\n\n# Use RDD transformations with predicted_RDD and movie_counts_RDD to yield an RDD with tuples of the form \n# (Movie ID, (Predicted Rating, number of ratings))\npredicted_with_counts_RDD  = (predicted_RDD\n                           .map(lambda x: (x[1],x[2])))\n\n# Use RDD transformations with Predicted_with_counts_RDD and movies_RDD to yield an RDD with tuples of the form\n# (Predicted Rating, Movie Name, number of ratings), for movies with more than 75 ratings\nratings_with_names_RDD = (predicted_with_counts_RDD\n                       .join(movie_counts_RDD)\n                      )\n\npredicted_highest_rated_movies = ratings_with_names_RDD.takeOrdered(20, key=lambda x: -x[0])\nprint('My highest rated movies as predicted (for movies with more than 75 reviews):\\n%s' %\n        '\\n'.join(map(str, predicted_highest_rated_movies)))"],"metadata":{"collapsed":false},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3168043217108019&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     17</span>                       )\n<span class=\"ansi-green-intense-fg ansi-bold\">     18</span> \n<span class=\"ansi-green-fg\">---&gt; 19</span><span class=\"ansi-red-fg\"> </span>predicted_highest_rated_movies <span class=\"ansi-blue-fg\">=</span> ratings_with_names_RDD<span class=\"ansi-blue-fg\">.</span>takeOrdered<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> key<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">-</span>x<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     20</span> print(&#39;My highest rated movies as predicted (for movies with more than 75 reviews):\\n%s&#39; %\n<span class=\"ansi-green-intense-fg ansi-bold\">     21</span>         &#39;\\n&#39;.join(map(str, predicted_highest_rated_movies)))\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">takeOrdered</span><span class=\"ansi-blue-fg\">(self, num, key)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1343</span>             <span class=\"ansi-green-fg\">return</span> heapq<span class=\"ansi-blue-fg\">.</span>nsmallest<span class=\"ansi-blue-fg\">(</span>num<span class=\"ansi-blue-fg\">,</span> a <span class=\"ansi-blue-fg\">+</span> b<span class=\"ansi-blue-fg\">,</span> key<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1344</span> \n<span class=\"ansi-green-fg\">-&gt; 1345</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> it<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">[</span>heapq<span class=\"ansi-blue-fg\">.</span>nsmallest<span class=\"ansi-blue-fg\">(</span>num<span class=\"ansi-blue-fg\">,</span> it<span class=\"ansi-blue-fg\">,</span> key<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>reduce<span class=\"ansi-blue-fg\">(</span>merge<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1346</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1347</span>     <span class=\"ansi-green-fg\">def</span> take<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> num<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">reduce</span><span class=\"ansi-blue-fg\">(self, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    883</span>             <span class=\"ansi-green-fg\">yield</span> reduce<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> iterator<span class=\"ansi-blue-fg\">,</span> initial<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    884</span> \n<span class=\"ansi-green-fg\">--&gt; 885</span><span class=\"ansi-red-fg\">         </span>vals <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>func<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    886</span>         <span class=\"ansi-green-fg\">if</span> vals<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    887</span>             <span class=\"ansi-green-fg\">return</span> reduce<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">,</span> vals<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    829</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    830</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 831</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    832</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    833</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2739.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2739.0 (TID 1568, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 508, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3168043217108019&gt;&#34;, line 11, in &lt;lambda&gt;\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:368)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:430)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2136)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:236)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2346)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:247)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor138.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 508, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3168043217108019&gt;&#34;, line 11, in &lt;lambda&gt;\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:368)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:645)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:430)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2136)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:236)\n</div>"]}}],"execution_count":49},{"cell_type":"code","source":["# You can save your model using the model.save()\nmodel.save(sc, \"./model_save.mfm\")"],"metadata":{"collapsed":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"code","source":["# And then, load it by doing:\nfrom pyspark.mllib.recommendation import MatrixFactorizationModel\ntest = MatrixFactorizationModel.load(sc, './model_save.mfm')\n\n# Why do we use MatrixFactorizationModel to load the model?\nprint(model)  # Because ALS models are an instance of the pyspark.mllib.recommendation.MatrixFactorizationModel class"],"metadata":{"collapsed":false},"outputs":[],"execution_count":51}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.9","nbconvert_exporter":"python","file_extension":".py"},"name":"4-spark-mllib","notebookId":3168043217107973},"nbformat":4,"nbformat_minor":0}
